Put your name and netid here
Zhecheng Sheng 
zs68

(1) Run the program BenchmarkForAutocomplete and copy/paste the 
results here this for #matches = 20
search	size	#match	binary	brute
	456976	20	0.1905	0.0089
a	17576	20	0.0034	0.0068
b	17576	20	0.0036	0.0068
c	17576	20	0.0034	0.0032
x	17576	20	0.0034	0.0037
y	17576	20	0.0039	0.0029
z	17576	20	0.0033	0.0032
aa	676	20	0.0001	0.0035
az	676	20	0.0001	0.0035
za	676	20	0.0001	0.0035
zz	676	20	0.0001	0.0033



(2) Run the program again for #matches = 10000, paste the results, 
and then make any conclusions about how the # matches 
effects the runtime. 
search	size	#match	binary	brute
	456976	10000	0.2967	0.0327
a	17576	10000	0.0033	0.0080
b	17576	10000	0.0033	0.0075
c	17576	10000	0.0033	0.0071
x	17576	10000	0.0037	0.0078
y	17576	10000	0.0033	0.0069
z	17576	10000	0.0035	0.0072
aa	676	10000	0.0001	0.0039
az	676	10000	0.0001	0.0039
za	676	10000	0.0001	0.0048
zz	676	10000	0.0001	0.0046
Answer: From these two outputs, we can conclude that this number of match k won't affect the runtime dramatically.


(3) Copy/paste the code from BruteAutocomplete.topMatches into the analysis.txt file. 
Explain what the Big-Oh complexity of the entire loop: 
for(Term t : myTerms) {...} 
is in terms of N, the number of elements in myTerms and 
M, the number of terms that match the prefix. 
Assume that every priority-queue operation runs in O(log k) time. 
Explain your answer which should be in terms of N, M, and k.
	public List<Term> topMatches(String prefix, int k) {
		if (k < 0) {
			throw new IllegalArgumentException("Illegal value of k:"+k);
		}
		
		// maintain pq of size k
		PriorityQueue<Term> pq = new PriorityQueue<Term>(10, new Term.WeightOrder());
		for (Term t : myTerms) {
			if (!t.getWord().startsWith(prefix))
				continue;0 
			if (pq.size() < k) {
				pq.add(t);
			} else if (pq.peek().getWeight() < t.getWeight()) {
				pq.remove();
				pq.add(t);
			}
		}
		int numResults = Math.min(k, pq.size());
		LinkedList<Term> ret = new LinkedList<>();
		for (int i = 0; i < numResults; i++) {
			ret.addFirst(pq.remove());
		}
		return ret;
	}
}
Answer: For the entire loop, the runtime complexity would be O(N + Mlogk), it takes O(N) to get all M terms that matched the prefix, and then each priority-queue operation takes O(logk), executing M times b/c there are M matches. 
So the overall runtime complexity is O(N + Mlogk).   


(4) Explain why the last for loop in BruteAutocomplete.topMatches 
uses a LinkedList (and not an ArrayList) 
AND why the PriorityQueue uses Term.WeightOrder to get 
the top k heaviest matches -- rather than 
using Term.ReverseWeightOrder.

Answer: Because it takes O(1) to add an element to front, do not have to shift;
And because every time priority queue takes elements from beginning to the end, so it uses a WeightOrder rather than ReverseWeightOrder.


(5) Explain what the runtime of the 
BinarySearchAutocomplete.topMatches code that you 
implemented is by copy/pasting the code below 
and explaining your answer in terms of N, M, and k.

	public List<Term> topMatches(String prefix, int k) {
		if(prefix==null) {
			throw new NullPointerException();
		}
		int l = prefix.length();
		Term base = new Term(prefix,0.0);
		Comparator<Term> comp = new Term.PrefixOrder(l);
		Comparator<Term> comp1 = new Term.ReverseWeightOrder();
		ArrayList<Term> list = new ArrayList<>();
		int low = BinarySearchLibrary.firstIndex(Arrays.asList(myTerms), base, comp);
		int high = BinarySearchLibrary.lastIndex(Arrays.asList(myTerms), base, comp);
		if(low ==-1| high==-1)return list;
		for(int i = low;i<=high;i++) {
			list.add(myTerms[i]);
		}
		Collections.sort(list, comp1);
		if(list.size()>k)return list.subList(0, k);
		else return list;
	}
Answer: Calling first index and last index cost O(logN) since they are binary search, copying the M matches to a new list cost O(M), sorting M elements by reverse weights cost O(MlogM). Extracting out the top k matches cost O(k).
So overall the runtime complexity is O(logN)+O(M)+O(Mlog(M))+O(k), since O(M)<O(MlogM), the final answer would be O(logN+MlogM+k).
